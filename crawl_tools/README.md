# Crawl Tools Directory

## üìã Daily Production Script

### **crawl_bot.py** ‚≠ê
**Purpose:** Main automated daily crawler - RUN THIS IN GITHUB ACTIONS

**What it does:**
- ‚úÖ Crawls domestic silver prices (Phu Quy brand) - last 7 days from giabac.vn
- ‚úÖ Crawls domestic gold prices (DOJI HN) - from 24h.com.vn via utils.py
- ‚úÖ Crawls SBV interbank rates - from State Bank of Vietnam API
- ‚úÖ Crawls ACB term deposit rates - from acb.com.vn

**Tables updated:**
- `vn_silver_phuquy_hist` (date, buy_price, sell_price)
- `vn_gold_24h_dojihn_hist` (date, buy_price, sell_price)
- `vn_sbv_interbankrate` (date, rates for 7 terms, trading volumes)
- `vn_bank_termdepo` (bank_code='ACB', date, term rates 1m-36m)

**Schedule:** Daily at 8:00 AM Vietnam time (GitHub Actions cron)

---

## üõ†Ô∏è Utility & Helper Files

### **utils.py**
**Purpose:** Shared utility functions

**Functions:**
- `crawl_gold_price_24h(date_str)` - Crawls DOJI HN gold prices from 24h.com.vn for specific date
- Used by crawl_bot.py

### **init_tables.py**
**Purpose:** Database table initialization script

**What it does:**
- Creates all required tables in Neon PostgreSQL database
- Tables: vn_gold_24h_dojihn_hist, vn_silver_phuquy_hist, vn_sbv_interbankrate, vn_bank_termdepo
- Run once during initial setup or when adding new tables

---

## üìä Historical Data Scripts

### **crawl_historical_backup.py**
**Purpose:** Backfill historical data (one-time use)

**What it does:**
- Crawls 30 days of silver price history from giabac.vn
- Crawls gold prices with date iteration (template code)
- Used for initial data population, not for daily runs

### **add_sample_data.py**
**Purpose:** Generate sample/test data

**What it does:**
- Creates 30 days of fake data for testing
- Useful for development/testing CSV download features
- DO NOT run in production

---

## üß™ Testing & Debug Scripts

### **test_bank_crawl.py**
**Purpose:** Test ACB and VCB website structure

**What it does:**
- Analyzes HTML tables on bank websites
- Used during development to understand page structure
- Not needed in production

### **fix_acb_crawler.py**
**Purpose:** Standalone ACB crawler test

**What it does:**
- Tests ACB term deposit crawler independently
- Used to debug and verify ACB parser before integrating into crawl_bot.py
- Not needed in production (logic now in crawl_bot.py)

### **test_banks.py**
**Purpose:** Test 6 banks that require JavaScript

**What it does:**
- Tests VietinBank, Agribank, MB, Techcombank, TPBank, VPBank
- All failed - these banks require browser automation
- Not functional - kept for reference

### **test_banks_advanced.py**
**Purpose:** Advanced testing with API endpoint attempts

**What it does:**
- Tests banks with multiple URLs (web + API endpoints)
- Checks for JSON responses and embedded script data
- Not functional - kept for debugging reference

### **manual_bank_check.py**
**Purpose:** Manual investigation of bank websites

**What it does:**
- Tests session-based scraping and cookie handling
- Investigates why banks don't return data
- Not functional - diagnostic tool only

---

## üö´ Failed Experiments (Browser Automation)

### **crawl_banks_selenium.py**
**Purpose:** Selenium-based crawler for 6 banks

**Status:** ‚ùå FAILED - ChromeDriver crashes
**Reason:** ChromeDriver not properly installed/configured
**Do not use**

### **crawl_banks_playwright.py**
**Purpose:** Playwright-based crawler for 6 banks

**Status:** ‚ùå FAILED - Pages don't load tables
**Reason:** Banks use enterprise CMS (IBM WebSphere Portal) with heavy anti-scraping
**Do not use**

### **debug_playwright.py**
**Purpose:** Debug Playwright with visible browser

**Status:** Incomplete debug script
**Do not use**

### **test_single_bank.py**
**Purpose:** Test single bank (Agribank) with extended timeouts

**Status:** ‚ùå FAILED - No content loaded
**Do not use**

---

## üìÅ Other Files

### **data_description.html**
**Purpose:** Technical documentation for macro crawling system

**What it contains:**
- Table schemas and descriptions
- Data sources and APIs
- VN30 stock list
- Future roadmap (NewsSentiment table)
- NOT a script - view in browser

### **agb_page.html** (generated)
**Purpose:** Saved HTML from Agribank test
**Auto-generated by test_single_bank.py - can be deleted**

---

## üéØ Summary - What to Run

| Script | When to Run | Purpose |
|--------|-------------|---------|
| **crawl_bot.py** | ‚≠ê **Daily (GitHub Actions)** | Production crawler |
| init_tables.py | Once (initial setup) | Create database tables |
| crawl_historical_backup.py | Once (backfill data) | Historical data import |
| add_sample_data.py | Never (dev only) | Test data generation |
| All other .py files | Never | Testing/debugging only |

---

## üîß GitHub Actions Configuration

Your `.github/workflows/daily-crawl.yml` should run:

```yaml
- name: Run daily crawler
  run: python crawl_tools/crawl_bot.py
```

**Dependencies required:**
```
pandas
requests
beautifulsoup4
sqlalchemy
psycopg2-binary
lxml
```

---

## üìù Notes

1. **Vietcombank (VCB)** crawler is commented out in crawl_bot.py - requires Selenium
2. **6 other banks** (VTB, AGB, MB, TCB, TPB, VPB) could not be crawled due to anti-scraping measures
3. Currently collecting data from **4 sources**: Silver, Gold, SBV Interbank, ACB Term Deposits
4. All test/debug files can be safely deleted from production deployment
