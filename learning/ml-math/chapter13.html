<!DOCTYPE html>
<html lang="vi">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 5: Calculus & Optimization - ML Mathematics</title>
  <link rel="stylesheet" href="../ml_book_shared-assets/styles.css">
  <link rel="stylesheet" href="../ml_book_shared-assets/custom-styles.css">
  <link rel="stylesheet" href="../ml_book_shared-assets/nav-styles.css">
  <link rel="stylesheet" href="../ml_book_shared-assets/chatgpt-styles.css">
  <link rel="stylesheet" href="../ml_book_shared-assets/responsive-fix.css">
</head>
<body>

<!-- Header -->
<div class="header">
  <h1>Machine Learning Mathematics</h1>

</div>

<!-- Sidebar Navigation -->
<div class="sidebar" id="sidebar">
  <!-- Will be populated by navigation.js -->
</div>

<!-- Toggle Sidebar Button (Mobile) -->
<button class="toggle-sidebar" id="toggleSidebar">☰ Menu</button>

<!-- Main Content -->
<div class="main-content">
  <div class="content-wrapper">

    <!-- Chapter Content -->
    <h1><span class="chapter-number">Chapter 5</span>Calculus & Optimization</h1>

    <h2>1. Why Calculus matters in ML</h2>
<p>
Hầu hết các thuật toán ML đều dựa trên việc tối ưu hóa (minimize hoặc maximize) một hàm mục tiêu.
Gradient descent và backpropagation - hai kỹ thuật cốt lõi - đều dựa trên đạo hàm và chain rule.
</p>

<h2>2. Derivatives in Machine Learning</h2>
<h3>2.1 What is a derivative?</h3>
<p>
Đạo hàm đo tốc độ thay đổi của hàm số theo biến.
Trong ML, đạo hàm cho biết hướng nào để điều chỉnh tham số để giảm loss.
</p>

<h3>2.2 Partial Derivatives</h3>
<p>
Khi hàm có nhiều biến (như trong ML), ta dùng partial derivative để đo sự thay đổi theo từng biến riêng lẻ.
</p>

<h2>3. Gradient Vector</h2>
<p>
Gradient là vector chứa tất cả partial derivatives của hàm.
Gradient luôn chỉ hướng tăng nhanh nhất của hàm.
</p>

<h2>4. Chain Rule</h2>
<p>
Chain rule cho phép tính đạo hàm của hàm hợp.
Đây là nền tảng của backpropagation trong Neural Networks.
</p>

<h2>5. Gradient Descent</h2>
<h3>5.1 Basic idea</h3>
<p>
Gradient descent là thuật toán cơ bản nhất để tối ưu hóa.
Ý tưởng: di chuyển ngược hướng gradient để giảm loss.
</p>

<h3>5.2 Update rule</h3>
<p>
θ_new = θ_old - α × ∇L(θ)
</p>
<ul>
  <li>α: learning rate</li>
  <li>∇L(θ): gradient của loss function</li>
</ul>

<h2>6. Variants of Gradient Descent</h2>
<ul>
  <li><b>Batch GD</b>: dùng toàn bộ dataset</li>
  <li><b>Stochastic GD</b>: dùng 1 sample mỗi lần</li>
  <li><b>Mini-batch GD</b>: dùng một batch nhỏ (phổ biến nhất)</li>
</ul>

<h2>7. Convexity and Local Minima</h2>
<p>
Hàm convex có một global minimum duy nhất.
Neural Networks không convex nên có thể bị kẹt ở local minima.
</p>

<h2>8. Common Interview Questions</h2>
<ul>
  <li>Why gradient descent works?</li>
  <li>What is the role of learning rate?</li>
  <li>How does backpropagation use chain rule?</li>
</ul>

<h2>9. Common Mistakes</h2>
<ul>
  <li>Không hiểu ý nghĩa hình học của gradient</li>
  <li>Chọn learning rate quá lớn hoặc quá nhỏ</li>
  <li>Nghĩ rằng gradient descent luôn tìm được global minimum</li>
</ul>

    <!-- Page Navigation -->
    <div class="page-navigation" id="pageNavigation">
      <!-- Will be populated by navigation.js -->
    </div>

  </div>
</div>

<script src="../ml_book_shared-assets/navigation.js"></script>
<script src="../ml_book_shared-assets/navigation-collapsible.js"></script>
<script src="../ml_book_shared-assets/math-config.js"></script>
<script src="../ml_book_shared-assets/navigation-collapsible.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</body>
</html>
