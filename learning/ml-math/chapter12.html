<!DOCTYPE html>
<html lang="vi">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 4: Eigendecomposition, PCA & Norms - ML Mathematics</title>
  <link rel="stylesheet" href="../ml_book_shared-assets/styles.css">
  <link rel="stylesheet" href="../ml_book_shared-assets/custom-styles.css">
  <link rel="stylesheet" href="../ml_book_shared-assets/nav-styles.css">
  <link rel="stylesheet" href="../ml_book_shared-assets/chatgpt-styles.css">
  <link rel="stylesheet" href="../ml_book_shared-assets/responsive-fix.css">
</head>
<body>

<!-- Header -->
<div class="header">
  <h1>Machine Learning Mathematics</h1>

</div>

<!-- Sidebar Navigation -->
<div class="sidebar" id="sidebar">
  <!-- Will be populated by navigation.js -->
</div>

<!-- Toggle Sidebar Button (Mobile) -->
<button class="toggle-sidebar" id="toggleSidebar">☰ Menu</button>

<!-- Main Content -->
<div class="main-content">
  <div class="content-wrapper">

    <!-- Chapter Content -->
    <h1><span class="chapter-number">Chapter 4</span>Eigendecomposition, PCA & Norms</h1>

    <h2>1. Vì sao Eigen và PCA quan trọng trong ML?</h2>
<p>
Khi dữ liệu có nhiều chiều và các feature tương quan mạnh, mô hình trở nên kém ổn định,
khó diễn giải và dễ overfitting. Eigen decomposition và PCA giúp ta:
</p>
<ul>
  <li>Xác định hướng thông tin quan trọng nhất</li>
  <li>Giảm chiều dữ liệu mà vẫn giữ phần lớn variance</li>
  <li>Loại bỏ nhiễu và redundancy giữa các feature</li>
</ul>

<h2>2. Eigenvalue và Eigenvector (intuition)</h2>

<h3>2.1 Định nghĩa trực quan</h3>
<p>
Với một phép biến đổi tuyến tính (matrix A), eigenvector là những vector mà khi nhân với A
chỉ bị kéo giãn hoặc co lại, nhưng không đổi hướng.
</p>
<p>
Hệ số kéo giãn đó chính là eigenvalue.
</p>

<h3>2.2 Ý nghĩa hình học</h3>
<p>
Eigenvector đại diện cho các "trục tự nhiên" của dữ liệu.
Eigenvalue càng lớn, trục đó càng chứa nhiều thông tin (variance).
</p>

<h2>3. Covariance Matrix và Eigen Decomposition</h2>
<p>
PCA bắt đầu từ covariance matrix của dữ liệu.
Covariance matrix mô tả mối quan hệ tuyến tính giữa các feature.
</p>
<p>
Eigen decomposition của covariance matrix giúp ta tìm ra:
</p>
<ul>
  <li>Eigenvectors: các trục mới</li>
  <li>Eigenvalues: mức độ quan trọng của từng trục</li>
</ul>

<h2>4. Principal Component Analysis (PCA)</h2>

<h3>4.1 PCA làm gì?</h3>
<p>
PCA tìm ra các hướng sao cho variance của dữ liệu sau khi chiếu lên là lớn nhất.
</p>

<h3>4.2 Các bước PCA (logic ML)</h3>
<ol>
  <li>Chuẩn hóa dữ liệu</li>
  <li>Tính covariance matrix</li>
  <li>Tính eigenvalues và eigenvectors</li>
  <li>Chọn top-k eigenvectors</li>
  <li>Project dữ liệu sang không gian mới</li>
</ol>

<h3>4.3 PCA giải quyết vấn đề gì trong thực tế?</h3>
<ul>
  <li>Giảm multicollinearity</li>
  <li>Tăng tốc training</li>
  <li>Ổn định model tuyến tính</li>
</ul>

<h2>5. Norms trong Machine Learning</h2>

<h3>5.1 Norm là gì?</h3>
<p>
Norm đo độ lớn của vector.
Trong ML, norm thường được dùng để đo độ phức tạp của model.
</p>

<h3>5.2 L2 Norm (Euclidean Norm)</h3>
<p>
L2 norm = sqrt(Σ wᵢ²)
</p>
<p>
L2 regularization (Ridge) phạt các trọng số lớn và giúp model ổn định.
</p>

<h3>5.3 L1 Norm</h3>
<p>
L1 norm = Σ |wᵢ|
</p>
<p>
L1 regularization (Lasso) tạo ra nhiều trọng số bằng 0 → feature selection.
</p>

<h3>5.4 So sánh L1 và L2</h3>
<ul>
  <li>L1: sparse, dễ diễn giải</li>
  <li>L2: ổn định, phân phối trọng số đều hơn</li>
</ul>

<h2>6. Interview Questions hay gặp</h2>
<ul>
  <li>Why PCA uses eigenvectors of covariance matrix?</li>
  <li>Why L1 leads to sparsity?</li>
  <li>When PCA should not be used?</li>
</ul>

<h2>7. Common Mistakes</h2>
<ul>
  <li>Dùng PCA mà không chuẩn hóa dữ liệu</li>
  <li>Hiểu PCA như feature selection (thực chất là feature transformation)</li>
  <li>Không phân biệt tác dụng của L1 và L2</li>
</ul>

    <!-- Page Navigation -->
    <div class="page-navigation" id="pageNavigation">
      <!-- Will be populated by navigation.js -->
    </div>

  </div>
</div>

<script src="../ml_book_shared-assets/navigation.js"></script>
<script src="../ml_book_shared-assets/navigation-collapsible.js"></script>
<script src="../ml_book_shared-assets/math-config.js"></script>
<script src="../ml_book_shared-assets/navigation-collapsible.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</body>
</html>
